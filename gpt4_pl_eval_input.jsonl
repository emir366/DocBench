{"question": "Does increasing specificity in responses lead to more nonsensical outputs?", "answer": "Yes.", "type": "text-only", "evidence": "\"At the extremes, the model produces only the most rare or the most common tokens...but overall produces less nonsensical outputs.\"", "sys_ans": "No, conditional training was not effective in controlling response-relatedness. According to the text: \"We found no improvement by controlling response-relatedness.\"", "file": "7"}
{"question": "Was the conditional training method effective in controlling response-relatedness?", "answer": "No.", "type": "text-only", "evidence": "\"However, we find this method ineffective...\"", "sys_ans": "The introduction of repetition control does not affect the question-asking rate at the highest control level (z=10).", "file": "7"}
{"question": "Are LIME and Alvarez-Melis and Jaakkola (2017) methods dependent on model properties?", "answer": "No", "type": "text-only", "evidence": "Other methods do not require any model properties. Examples include LIME (Ribeiro et al., 2016) and Alvarez-Melis and Jaakkola (2017).", "sys_ans": "The annotations for the BoolQ dataset were collected on the PyTorch platform.", "file": "6"}
{"question": "On which platform were the annotations for the BoolQ dataset collected?", "answer": "Amazon Mechanical Turk (MTurk).", "type": "text-only", "evidence": "Paragraph B3, \"We used Amazon Mechanical Turk (MTurk) to collect...\"", "sys_ans": "Based on the table, the difference between the highest and lowest AUPRC values is 0.46 - 0.05 = 0.41. This corresponds to the model combinations Lei et al. (2016) and lei et al. (2016) ().", "file": "6"}
{"question": "Which model combination for the Evidence Inference dataset has the highest AUPRC value?", "answer": "GloVe + LSTM - Attention has the highest AUPRC value at 0.506 for the Evidence Inference dataset.", "type": "multimodal-t", "evidence": "The AUPRC values for all model combinations in the Evidence Inference dataset are listed in the AUPRC column, and GloVe + LSTM - Attention has the highest value among them.", "sys_ans": "The paper has 11 pages (excluding references and appendix).", "file": "6"}
{"question": "How many Facebook comments were provided in the dataset for training purposes?", "answer": "12,000 Facebook comments.", "type": "text-only", "evidence": "\"The participants of the shared task were provided with a dataset of 12,000 randomly sampled Facebook comments for training...\"", "sys_ans": "The number of Facebook comments provided in the dataset for training purposes is **not mentioned** in the given document.", "file": "1"}
{"question": "How many categories of aggression were participants asked to classify texts into?", "answer": "3 categories: overt aggression, covert aggression, and non-aggression.", "type": "text-only", "evidence": "\"It only involved classification of the texts into 3 categories - overt aggression, covert aggression and non-aggression.\"", "sys_ans": "The number of categories of aggression that participants were asked to classify texts into is also **not mentioned** in the given document. It appears that the workshop focused on general aggression detection and didn't specify specific categories for classification.", "file": "1"}
{"question": "What was the F1 score achieved by single-paragraph BERT in the open-domain setting?", "answer": "38.06.", "type": "text-only", "evidence": "\"Single-paragraph BERT achieves 38.06 F1 in the open-domain setting.\"", "sys_ans": "The answer is: 73.5", "file": "8"}
{"question": "What F1 score did the single-paragraph BERT model achieve in the distractor setting on HOTPOTQA?", "answer": "67.08.", "type": "text-only", "evidence": "\"Our single-paragraph BERT model achieves 67.08 F1.\"", "sys_ans": "What F1 score did the single-paragraph BERT model achieve in the distractor setting on HOTPOTQA?", "file": "8"}
{"question": "Which model has the highest Distractor F1 score according to Table 1?", "answer": "BERT Plus Cognitive Graph has the highest Distractor F1 score with 69.76.", "type": "multimodal-t", "evidence": "Looking through the \"Distractor F1\" column, BERT Plus Cognitive Graph has the highest numerical value.", "sys_ans": "Which model has the highest Distractor F1 score according to Table 1?", "file": "8"}
{"question": "What is the F1 score of the single-paragraph BERT model in the setting with 500 open-domain paragraphs?", "answer": "39.12.", "type": "multimodal-t", "evidence": "The F1 score for the setting with 500 open-domain paragraphs is directly stated in the table.", "sys_ans": "What is the F1 score of the single-paragraph BERT model in the setting with 500 open-domain paragraphs?", "file": "8"}
{"question": "By how much does the F1 score improve when the gold paragraph is added to the open-domain setting with 500 paragraphs?", "answer": "The F1 score improves by 14.00 when the gold paragraph is added.", "type": "multimodal-t", "evidence": "The F1 score improves from 39.12 to 53.12 when the gold paragraph is provided. The improvement can be calculated by subtracting the former from the latter (53.12 - 39.12 = 14.00).", "sys_ans": "By how much does the F1 score improve when the gold paragraph is added to the open-domain setting with 500 paragraphs?", "file": "8"}
{"question": "According to Figure 2, how does the model decide which answer to select?", "answer": "The model selects the answer from the paragraph with the lowest y_empty score.", "type": "multimodal-f", "evidence": "This question requires comprehension of the specific criterion for answer selection as illustrated in the figure where the pointer indicates the lowest y_empty score.", "sys_ans": "According to Figure 2, how does the model decide which answer to select?", "file": "8"}
{"question": "What is the reported issue with sequence generation models in dialogue tasks?", "answer": "They tend to produce short, generic sentences containing frequent words.", "type": "text-only", "evidence": "In the introduction, several authors report that sequence generation models \"produce short, generic sentences containing frequent words.\"", "sys_ans": "According to the uploaded text, Seq2Seq models are shown to produce shorter sentences when compared to human responses.", "file": "4"}
{"question": "Is the Seq2Seq model shown to produce longer or shorter sentences when compared to human responses?", "answer": "Shorter.", "type": "text-only", "evidence": "\"Seq2Seq models are known to produce short sentences with more common words than humans.\"", "sys_ans": "The method that scored the highest in the Engagingness metric is RetrieveNRefine++.", "file": "4"}
{"question": "How many total evaluations were collected for the comparison between RetrieveNRefine++ (retrieved) vs. Seq2Seq?", "answer": "total of 626 evaluations were collected for RetrieveNRefine++ (retrieved) vs. Seq2Seq (290 A Wins + 249 B Wins + 87 Ties = 626).", "type": "multimodal-t", "evidence": "This question involves performing a simple arithmetic operation by summing up the number of wins and ties for a specific model pair.", "sys_ans": "According to the uploaded text, there is no specific answer provided regarding which model produces the longest sentences in terms of word count; however, it does mention that our RetNRef++ model \"has similar statistics to human utterances\".", "file": "4"}
{"question": "Which method scored the highest in the Engagingness metric?", "answer": "RetrieveNRefine++ scored the highest in the Engagingness metric with a score of 3.80.", "type": "multimodal-t", "evidence": "The question asks to identify the highest score in a particular column, which is straightforward to answer by scanning the Engagingness column.", "sys_ans": "The first sentence on page 5 is: \"In conclusion...\"", "file": "4"}
{"question": "What training data were used for the models in this study?", "answer": "Common Crawl paragraphs of arbitrary length.", "type": "text-only", "evidence": "The text states \"Training examples for our model are Common Crawl paragraphs of arbitrary length.\"", "sys_ans": "According to Figure 3, the largest amount of Common Crawl tokens (18B) resulted in the highest average accuracy across all GLUE tasks.", "file": "3"}
{"question": "Which training data size from the Common Crawl resulted in the highest average accuracy across all GLUE tasks?", "answer": "The 18,000 million tokens (M tok) training data size from the Common Crawl resulted in the highest average accuracy of 81.3 across all GLUE tasks.", "type": "multimodal-t", "evidence": "The question asks for the highest average accuracy across all GLUE tasks for a specific data size. This is found by examining the 'Avg' column and looking for the highest number in the 'ccrawl' section.", "sys_ans": "According to Table 4, the BERT models have the highest test F1 score for constituency parsing.", "file": "3"}
{"question": "How does the CNN Base model's performance on the SST-2 task compare to the OpenAI GPT?", "answer": "The CNN Base model has an SST-2 score of 93.6, which is higher than OpenAI GPT's score of 91.3.", "type": "multimodal-t", "evidence": "Direct comparison between the SST-2 scores listed for each model shows that the CNN Base model outperforms OpenAI GPT on this specific task.", "sys_ans": "The combination (comb) layers in Figure 2 are not mentioned as having a specific purpose.", "file": "3"}
{"question": "Which model has the highest test F1 score according to Table 4?", "answer": "The CNN Large with fine-tune model has the highest test F1 score of 95.6.", "type": "multimodal-t", "evidence": "This is a comparison question that requires examining all listed models to determine which one has the highest score in the test F1 column.", "sys_ans": "The paper publishes at the Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing.", "file": "3"}
{"question": "Which language pair has the highest accuracy (Acc) score for Google Translate according to Table 2?", "answer": "French (FR) has the highest accuracy score for Google Translate with a score of 63.6.", "type": "multimodal-t", "evidence": "By comparing the Acc scores in the Google Translate column for all language pairs, French (FR) has the highest listed value.", "sys_ans": "(Spanish has an Acc score of 64.7% in Table 2)", "file": "2"}
{"question": "Based on Figure 2, which language exhibited the greatest bias in accuracy between stereotypical and non-stereotypical gender roles when using Google Translate?", "answer": "HE (Hebrew) exhibited the greatest bias in accuracy between stereotypical and non-stereotypical gender roles when using Google Translate, with an accuracy of 76% for stereotypical roles and 38% for non-stereotypical roles, resulting in a difference of 38 percentage points.", "type": "multimodal-f", "evidence": "The greatest bias can be identified by", "sys_ans": "(French has a difference of 34.4% points between stereotypical and non-stereotypical gender roles in Figure 2)", "file": "2"}
{"question": "Which language showed the greatest improvement in gender prediction accuracy when adjectives were added?", "answer": "Russian (RU) showed the greatest improvement with an increase of +11.2% in gender prediction accuracy.", "type": "multimodal-t", "evidence": "Comparing the \u0394 values for each language, Russian has the highest increase.", "sys_ans": "(Spanish had an Acc score of 63.1% without adjectives and 73.2% with adjectives, according to Table 3)", "file": "2"}
{"question": "How many references are cited in the paper?", "answer": "28", "type": "meta-data", "evidence": "", "sys_ans": "According to my count, there are approximately 4680 words (excluding references) in the paper.", "file": "2"}
{"question": "What is one limitation of using Google Ngrams for studying semantic shifts?", "answer": "Google Ngrams is limited because it does not contain full texts.", "type": "text-only", "evidence": "The text states that \"Google Ngrams is inherently limited in that it does not contain full texts.\"", "sys_ans": "**The use of diachronic word embeddings for studying the dynamics of word meaning has resulted in several hypothesized \"laws\" of semantic change**. This is the main content of section 4.2.", "file": "5"}
